Metadata-Version: 2.4
Name: agent-memory-os
Version: 0.1.0
Summary: A comprehensive memory layer for AI agents providing persistent, semantic + episodic memory
Home-page: https://github.com/your-org/agent-memory-os
Author: Agent Memory OS Contributors
Author-email: contributors@agent-memory-os.dev
Project-URL: Bug Tracker, https://github.com/your-org/agent-memory-os/issues
Project-URL: Documentation, https://github.com/your-org/agent-memory-os#readme
Project-URL: Source Code, https://github.com/your-org/agent-memory-os
Keywords: ai,agents,memory,langchain,langgraph,pinecone,postgresql,vector-search,semantic-memory,episodic-memory,machine-learning,artificial-intelligence
Platform: any
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
Classifier: Framework :: FastAPI
Classifier: Framework :: LangChain
Classifier: Framework :: LangGraph
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pydantic>=2.0.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: langchain>=0.1.0
Requires-Dist: langchain-community>=0.0.10
Requires-Dist: langchain-core>=0.1.0
Requires-Dist: langgraph>=0.5.0
Requires-Dist: fastapi>=0.104.0
Requires-Dist: uvicorn[standard]>=0.24.0
Requires-Dist: aiohttp>=3.8.0
Requires-Dist: requests>=2.28.0
Requires-Dist: pinecone>=7.0.0
Requires-Dist: psycopg2-binary>=2.9.0
Requires-Dist: mcp>=1.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: pytest>=7.0.0
Requires-Dist: pytest-asyncio>=0.21.0
Requires-Dist: pytest-cov>=4.0.0
Requires-Dist: black>=23.0.0
Requires-Dist: flake8>=6.0.0
Requires-Dist: mypy>=1.0.0
Requires-Dist: bandit>=1.7.0
Requires-Dist: safety>=2.0.0
Requires-Dist: pydocstyle>=6.0.0
Requires-Dist: build>=0.10.0
Requires-Dist: twine>=4.0.0
Requires-Dist: wheel>=0.40.0
Provides-Extra: langchain
Requires-Dist: langchain>=0.1.0; extra == "langchain"
Requires-Dist: langchain-community>=0.0.10; extra == "langchain"
Requires-Dist: langchain-core>=0.1.0; extra == "langchain"
Provides-Extra: langgraph
Requires-Dist: langgraph>=0.0.20; extra == "langgraph"
Provides-Extra: pinecone
Requires-Dist: pinecone>=7.0.0; extra == "pinecone"
Provides-Extra: postgresql
Requires-Dist: psycopg2-binary>=2.9.0; extra == "postgresql"
Provides-Extra: api
Requires-Dist: fastapi>=0.100.0; extra == "api"
Requires-Dist: uvicorn[standard]>=0.20.0; extra == "api"
Requires-Dist: httpx>=0.24.0; extra == "api"
Provides-Extra: mcp
Requires-Dist: mcp>=1.0.0; extra == "mcp"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: langchain>=0.1.0; extra == "dev"
Requires-Dist: langchain-community>=0.0.10; extra == "dev"
Requires-Dist: langchain-core>=0.1.0; extra == "dev"
Requires-Dist: langgraph>=0.0.20; extra == "dev"
Requires-Dist: pinecone>=7.0.0; extra == "dev"
Requires-Dist: psycopg2-binary>=2.9.0; extra == "dev"
Requires-Dist: fastapi>=0.100.0; extra == "dev"
Requires-Dist: uvicorn[standard]>=0.20.0; extra == "dev"
Requires-Dist: httpx>=0.24.0; extra == "dev"
Requires-Dist: mcp>=1.0.0; extra == "dev"
Provides-Extra: all
Requires-Dist: langchain>=0.1.0; extra == "all"
Requires-Dist: langchain-community>=0.0.10; extra == "all"
Requires-Dist: langchain-core>=0.1.0; extra == "all"
Requires-Dist: langgraph>=0.0.20; extra == "all"
Requires-Dist: pinecone>=7.0.0; extra == "all"
Requires-Dist: psycopg2-binary>=2.9.0; extra == "all"
Requires-Dist: fastapi>=0.100.0; extra == "all"
Requires-Dist: uvicorn[standard]>=0.20.0; extra == "all"
Requires-Dist: httpx>=0.24.0; extra == "all"
Requires-Dist: mcp>=1.0.0; extra == "all"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license-file
Dynamic: platform
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# agent-memory-os

Agent Memory OS is an open-source memory layer for AI agents — providing persistent, semantic + episodic memory that spans time, tools, and multi-agent systems.

# 🧠 Agent Memory OS

## Overview

Modern LLM agents are smart — but forgetful.

They can answer questions, write code, and simulate workflows, but as soon as the session ends, they forget everything. This creates brittle experiences, limits long-term learning, and makes it hard to scale AI assistants in real-world use cases.

**Agent Memory OS** solves this by offering a plug-and-play memory architecture that adds persistent, structured memory to your AI agents — across sessions, across tasks, and even across teams of agents.

---

## 🚧 The Problem

LLM agents today lack true long-term memory.

- ❌ Agents forget previous conversations or context unless manually re-fed.
- ❌ Developers have to rebuild memory pipelines from scratch for every app.
- ❌ No standard for sharing memory across agents or workflows.
- ❌ Vector search ≠ structured memory with episodic or semantic context.
- ❌ No temporal awareness — agents don't "remember what happened when."

This results in fragile systems and poor user experience.

---

## ✅ The Solution

**Agent Memory OS** is a developer-first memory framework for agentic systems.

- 🧩 Unified **episodic**, **semantic**, and **temporal** memory
- 🔁 Works across **LangChain**, **CrewAI**, **LangGraph**, **REST API**, and custom agents
- 🧠 Designed for **multi-agent collaboration** with shared context
- 📚 Easily plug into your LLM pipeline via a simple SDK, API, or HTTP
- ⏱️ Supports time-aware memory recall and timeline reconstruction

---

## 🛠️ Who It's For

- AI engineers building assistants, copilots, and autonomous workflows
- Teams using LangGraph, CrewAI, LangChain, or REST APIs
- Developers building apps that need persistent, evolving memory

---

## 📋 Implementation Status

### ✅ **Fully Implemented**

#### Core Memory System

- **MemoryManager** - Main interface for memory operations
- **MemoryEntry** - Data model for individual memories
- **MemoryType** - Enum for episodic, semantic, and temporal memory types
- **SQLiteStore** - Persistent storage backend with full CRUD operations
- **Basic embedding generation** - Hash-based embeddings for demo purposes

#### Memory Operations

- ✅ Add memories with metadata and timestamps
- ✅ Search memories by content (text-based LIKE search)
- ✅ Filter memories by type, agent_id, session_id
- ✅ Retrieve episodic memories for specific agents/sessions
- ✅ Get chronological timeline of memories
- ✅ Memory persistence across sessions

#### Utilities

- ✅ Time utilities for timestamp formatting and parsing
- ✅ Embedding utilities with cosine similarity calculations
- ✅ Memory serialization (to_dict/from_dict)

#### Testing & Examples

- ✅ Comprehensive unit tests (9/9 passing)
- ✅ Working demo with CrewAI integration example
- ✅ Memory persistence demonstration

#### LangChain Integration ✅ **COMPLETE**

- ✅ **MemoryChain** - LangChain chain with automatic memory storage and retrieval
- ✅ **MemoryTool** - LangChain tool for agents to store facts and search memories
- ✅ **MemoryCallbackHandler** - Automatic memory storage during chain/agent execution
- ✅ **MemoryAwareAgent** - Wrapper for any LangChain agent with memory capabilities
- ✅ **Complete integration demo** - Working example with all components
- ✅ **Memory persistence** - Memories survive across sessions and agent restarts
- ✅ **Semantic search** - Basic text-based search for relevant memories
- ✅ **Timeline retrieval** - Get chronological history of agent activities

#### LangGraph Integration ✅ **NEW - COMPLETE**

- ✅ **MemoryGraph** - Complete memory-aware LangGraph workflow wrapper
- ✅ **MemoryState** - State class with memory capabilities for LangGraph
- ✅ **MemoryNode** - Node that provides memory operations in workflows
- ✅ **MemoryToolNode** - Tools that can be used in any LangGraph workflow
- ✅ **create_memory_tools** - Helper function to create memory tools
- ✅ **Complete integration demo** - 4 comprehensive demos showing different usage patterns
- ✅ **Memory persistence** - Memories survive across graph executions
- ✅ **Agent-specific memory isolation** - Each agent/workflow has isolated memory
- ✅ **ToolNode compatibility** - Works with latest LangGraph ToolNode API
- ✅ **Dict-based state management** - Maximum compatibility with LangGraph

#### REST API Integration ✅ **NEW - COMPLETE**

- ✅ **FastAPI-based REST API** for remote memory access
- ✅ **Full CRUD**: create, update, delete, search, and list memories
- ✅ **Agent-specific endpoints** and statistics
- ✅ **Health and status endpoints**
- ✅ **Python client library** (`MemoryAPIClient`, `AsyncMemoryAPIClient`)
- ✅ **Comprehensive demo** (`examples/api_demo.py`)
- ✅ **Interactive docs** at `/docs` and `/redoc`

#### Web UI Integration ✅ **NEW - COMPLETE**

- ✅ **Modern web interface** for memory visualization and management
- ✅ **Real-time statistics** and memory type breakdowns
- ✅ **Interactive search and filtering** by type, agent, and importance
- ✅ **Memory detail views** with full metadata
- ✅ **Create and delete memories** directly from the UI
- ✅ **Responsive design** that works on desktop and mobile
- ✅ **Auto-refresh** and live updates
- ✅ **Sample data demo** (`examples/web_ui_demo.py`)

#### MCP (Model Context Protocol) Integration ✅ **NEW - COMPLETE**

- ✅ **Complete MCP server implementation** for Claude Desktop integration
- ✅ **Memory tools**: search, create, update, delete, and get statistics
- ✅ **Memory resources**: individual memories and summary overview
- ✅ **Multiple storage backends**: SQLite, Pinecone, and PostgreSQL support
- ✅ **CLI interface**: `agent-memory-mcp` command with full configuration options
- ✅ **stdio and HTTP modes**: Flexible deployment for different MCP clients
- ✅ **Configuration files**: Ready-to-use configs for Claude Desktop
- ✅ **Comprehensive demo** (`examples/mcp_server_demo.py`)

### 🚧 **Partially Implemented**

#### Semantic Search

- ✅ **Vector similarity search** with Pinecone integration
- ✅ **Model-based embeddings** using `llama-text-embed-v2` (1024 dimensions)
- ✅ **High-quality semantic search** across all memory types
- ✅ **Fallback text search** for SQLite backend
- ✅ **Cosine similarity** calculations for relevance scoring

#### LangChain Integration

- ⚠️ **MemoryAwareAgent callback compatibility** - Minor callback handler issue with LLMChain (cosmetic, doesn't affect core functionality)
- ⚠️ **Deprecation warnings** - Some LangChain APIs are deprecated but still functional

### ❌ **Not Yet Implemented**

#### Advanced Features

- ❌ Vector similarity search using embeddings
- ❌ Integration with proper embedding models (OpenAI, sentence-transformers)
- ❌ Memory compression and summarization
- ❌ Memory importance scoring
- ❌ Automatic memory cleanup/archival
- ❌ Multi-agent memory sharing protocols

#### Integrations

- ❌ Direct CrewAI integration (only example code provided)

#### Storage Backends

- ✅ **Pinecone Integration** - Vector database with semantic search
  - ✅ Model-based embeddings with `llama-text-embed-v2`
  - ✅ 1024-dimensional vectors for high-quality semantic search
  - ✅ Automatic index creation and management
  - ✅ Cross-session memory persistence
  - ✅ Full CRUD operations with metadata
- ✅ **PostgreSQL Integration** - Production-ready relational database
  - ✅ Full-text search with PostgreSQL's tsvector/tsquery
  - ✅ JSONB support for flexible metadata storage
  - ✅ ACID compliance and transaction support
  - ✅ High-performance indexing and querying
  - ✅ Scalable for large memory datasets
- ✅ **Store Factory** - Adapter pattern for multiple backends
- ✅ **Auto-detection** - Automatically chooses SQLite, PostgreSQL, or Pinecone
- ❌ Redis backend
- ❌ Other vector databases (Weaviate, Qdrant, etc.)
- ❌ Cloud storage options

#### Advanced Memory Features

- ❌ Memory relationships and linking
- ❌ Memory versioning and history
- ❌ Memory privacy and access control
- ❌ Memory export/import functionality
- ❌ Memory analytics and insights

---

## 🔍 MVP Goal

Enable a basic CrewAI, LangChain, LangGraph, or REST API agent to:

- ✅ Remember key facts across sessions
- ✅ Recall specific past interactions (episodic)
- ✅ Retrieve semantically relevant info when prompted (basic text search)
- ✅ Track events across time
- ✅ Access and manage memory remotely via HTTP or Python client

**✅ MVP COMPLETED** - All core memory functionality is working with LangChain, LangGraph, and REST API integrations!

---

## 📦 Project Structure

```
agent-memory-os/
├── agent_memory_sdk/          # Core SDK
│   ├── __init__.py
│   ├── memory.py              # MemoryManager class
│   ├── models.py              # Data models (MemoryEntry, MemoryType)
│   ├── store/                 # Storage backends
│   │   ├── __init__.py
│   │   ├── base_store.py      # Abstract base class
│   │   ├── sqlite_store.py    # SQLite storage implementation
│   │   ├── pinecone_store.py  # Pinecone vector storage
│   │   └── store_factory.py   # Store factory for backend selection
│   ├── utils/                 # Utility functions
│   │   ├── __init__.py
│   │   ├── embedding_utils.py # Embedding generation and similarity
│   │   └── time_utils.py      # Time formatting utilities
│   ├── api/                   # REST API (FastAPI server, client, models)
│   │   ├── __init__.py
│   │   ├── server.py          # FastAPI app and endpoints
│   │   ├── models.py          # Pydantic models for API
│   │   ├── client.py          # Python/async client for API
│   │   └── static/            # Web UI static files
│   │       ├── index.html     # Main web UI page
│   │       ├── styles.css     # Web UI styles
│   │       └── app.js         # Web UI JavaScript
│   └── integrations/          # Framework integrations
│       ├── __init__.py
│       ├── langchain/         # LangChain integration
│       │   ├── __init__.py
│       │   ├── memory_chain.py        # MemoryChain class
│       │   ├── memory_tool.py         # MemoryTool class
│       │   ├── memory_callback.py     # MemoryCallbackHandler
│       │   └── memory_agent.py        # MemoryAwareAgent wrapper
│       ├── langgraph/         # LangGraph integration
│       │   ├── __init__.py
│       │   ├── memory_graph.py        # MemoryGraph class
│       │   ├── memory_state.py        # MemoryState class
│       │   ├── memory_node.py         # MemoryNode class
│       │   └── memory_tool_node.py    # MemoryToolNode class
│       └── mcp/               # MCP (Model Context Protocol) integration
│           ├── __init__.py
│           └── memory_mcp_server.py   # MCP server implementation
├── examples/                  # Integration examples
│   ├── crewai_memory_demo.py  # CrewAI integration demo
│   ├── langchain_memory_demo.py # LangChain integration demo
│   ├── langgraph_memory_demo.py # LangGraph integration demo
│   ├── mcp_server_demo.py     # MCP server demo
│   ├── api_demo.py            # REST API demo (HTTP, client, async)
│   └── web_ui_demo.py         # Web UI demo with sample data
├── tests/                     # Unit tests
│   ├── __init__.py
│   └── test_memory.py         # Comprehensive test suite
├── requirements.txt           # Dependencies
└── README.md                 # This file
```

---

## 🚀 Quick Start

### Installation

```bash
git clone <repository>
cd agent-memory-os
pip install -r requirements.txt
```

### Run the REST API Server

```bash
# Run from the project root directory
python run_api.py --host 127.0.0.1 --port 8000
```

**API Endpoints:**

- 📚 **Docs**: http://127.0.0.1:8000/docs (Interactive API documentation)
- 🔍 **ReDoc**: http://127.0.0.1:8000/redoc (Alternative API docs)
- ❤️ **Health**: http://127.0.0.1:8000/health (Server status)
- 🎨 **Web UI**: http://127.0.0.1:8000 (Interactive web interface)

**Features:**

- Full CRUD operations for memories
- Semantic search and filtering
- Agent-specific endpoints
- Real-time statistics and health monitoring

### Use the Python Client

```python
from agent_memory_sdk.api import MemoryAPIClient
from agent_memory_sdk.api.models import MemoryCreateRequest
from agent_memory_sdk.models import MemoryType

with MemoryAPIClient("http://localhost:8000") as client:
    req = MemoryCreateRequest(content="Hello", memory_type=MemoryType.SEMANTIC)
    memory = client.create_memory(req)
    print(memory)
```

### Use the Async Python Client

```python
import asyncio
from agent_memory_sdk.api import AsyncMemoryAPIClient
from agent_memory_sdk.api.models import MemoryCreateRequest
from agent_memory_sdk.models import MemoryType

async def main():
    async with AsyncMemoryAPIClient("http://localhost:8000") as client:
        req = MemoryCreateRequest(content="Async hello", memory_type=MemoryType.EPISODIC)
        memory = await client.create_memory(req)
        print(memory)

asyncio.run(main())
```

### Try the REST API Demo

```bash
python examples/api_demo.py
```

### Try the Web UI Demo

```bash
# Run from any directory - the script will automatically find the project root
python examples/web_ui_demo.py
```

This will:

1. **Create sample memory data** with various types and agents
2. **Launch the web UI server** at http://localhost:8000
3. **Open your browser** automatically to the web interface
4. **Show interactive features** like search, filtering, and memory management

**Web UI Features:**

- 📊 Real-time memory statistics and breakdowns
- 🔍 Interactive search and filtering by type, agent, and importance
- 📝 Create and delete memories directly from the interface
- 📱 Responsive design for desktop and mobile
- 🔄 Auto-refresh and live updates

### Basic Usage (SDK)

```python
from agent_memory_sdk import MemoryManager, MemoryType

# Initialize memory manager (auto-detects SQLite or Pinecone)
memory_manager = MemoryManager()

# Add memories
memory_manager.add_memory(
    content="User prefers Python programming",
    memory_type=MemoryType.SEMANTIC,
    agent_id="my_agent",
    importance=8.0,
    tags=["preference", "programming"]
)

# Search memories (semantic search with Pinecone, text search with SQLite)
results = memory_manager.search_memory("Python")
```

### Storage Backends

The system supports multiple storage backends:

#### SQLite (Default - Local)

```python
# Local SQLite storage
memory_manager = MemoryManager(store_type="sqlite", db_path="memories.db")
```

#### Pinecone (Cloud - Vector Search)

```bash
# Set environment variables
export PINECONE_API_KEY="your-api-key"
export PINECONE_ENVIRONMENT="your-environment"
```

```python
# Pinecone vector storage with semantic search
memory_manager = MemoryManager(store_type="pinecone", index_name="my-memories")
```

#### PostgreSQL (Production - Relational Database)

```bash
# Set environment variables
export POSTGRESQL_HOST="localhost"
export POSTGRESQL_PORT="5432"
export POSTGRESQL_DATABASE="agent_memory"
export POSTGRESQL_USER="postgres"
export POSTGRESQL_PASSWORD="your_password"

# OR use connection string
export POSTGRESQL_CONNECTION_STRING="postgresql://user:pass@host:port/db"
```

```python
# PostgreSQL storage with full-text search
memory_manager = MemoryManager(store_type="postgresql")

# Or with explicit connection parameters
memory_manager = MemoryManager(
    store_type="postgresql",
    host="localhost",
    port="5432",
    database="agent_memory",
    user="postgres",
    password="your_password"
)
```

### Try the Pinecone Demo

```bash
# Set your Pinecone credentials
export PINECONE_API_KEY="your-api-key"
export PINECONE_ENVIRONMENT="your-environment"

# Run the demo
python examples/pinecone_memory_demo.py
```

**Features demonstrated:**

- Model-based embeddings with `llama-text-embed-v2`
- High-quality semantic search
- Memory persistence across sessions
- Comparison between SQLite and Pinecone backends

### LangChain Integration

```python
from agent_memory_sdk import MemoryChain, MemoryManager
from langchain_community.llms import OpenAI

# Initialize memory manager
memory_manager = MemoryManager()

# Create memory-aware chain
memory_chain = MemoryChain(
    memory_manager=memory_manager,
    llm=OpenAI(),
    agent_id="my_agent"
)

# Use with memory persistence
result = memory_chain.invoke({"input": "What do you remember about me?"})
print(result["output"])
```

### LangGraph Integration

```python
from agent_memory_sdk import MemoryGraph, MemoryManager
from langgraph.graph import StateGraph, END

# Initialize memory manager
memory_manager = MemoryManager()

# Create memory-aware graph
memory_graph = MemoryGraph(
    memory_manager=memory_manager,
    agent_id="my_agent",
    session_id="session_1"
)

# Create a simple graph
graph = memory_graph.create_graph()

# Add a processing node
def process_input(state):
    input_text = state.get("input", "")
    memory_graph.memory_manager.add_memory(
        content=f"Processed: {input_text}",
        memory_type=MemoryType.EPISODIC,
        agent_id=memory_graph.agent_id,
        session_id=memory_graph.session_id
    )
    return {"output": f"Processed: {input_text}"}

graph.add_node("process", process_input)
graph.set_entry_point("process")
graph.add_edge("process", END)

# Compile and run
compiled_graph = graph.compile()
result = compiled_graph.invoke({"input": "Hello, this is my first memory!"})
```

### Memory Tools in LangGraph

```python
from agent_memory_sdk import create_memory_tools, MemoryManager
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, AIMessage
import uuid

# Create memory tools
memory_tools = create_memory_tools(
    memory_manager=MemoryManager(),
    agent_id="my_agent"
)

# Create tool node
tool_node = ToolNode(memory_tools)

# Use with proper message format
tool_call = {
    "name": "store_memory",
    "args": {"content": "I learned something new", "memory_type": "semantic"},
    "id": str(uuid.uuid4()),
    "type": "tool_call"
}

state = {
    "messages": [
        HumanMessage(content="Store a memory"),
        AIMessage(content="", tool_calls=[tool_call])
    ]
}

result = tool_node.invoke(state)
```

### MCP (Model Context Protocol) Integration

Agent Memory OS provides a complete MCP server implementation that allows AI assistants like Claude Desktop to access and manage memories through the standardized Model Context Protocol.

#### Quick Start with Claude Desktop

1. **Install the MCP server:**

```bash
pip install agent-memory-os[mcp]
```

2. **Add to Claude Desktop configuration:**

   Copy the configuration from `mcp-server-config.json` to your Claude Desktop config:

   ```json
   {
     "mcpServers": {
       "agent-memory-os": {
         "command": "agent-memory-mcp",
         "args": ["--stdio"],
         "env": {
           "STORE_TYPE": "sqlite",
           "DB_PATH": "agent_memory.db"
         }
       }
     }
   }
   ```

3. **Restart Claude Desktop** and start using memory capabilities!

#### Available MCP Tools

The MCP server provides these tools to AI assistants:

- **`search_memories`** - Search for memories using semantic search and filters
- **`create_memory`** - Create new memory entries with metadata
- **`get_memory_stats`** - Get statistics about stored memories
- **`delete_memory`** - Delete specific memories by ID
- **`update_memory`** - Update existing memory content and metadata

#### Available MCP Resources

- **`memory://*`** - Individual memory resources with full metadata
- **`memory://summary`** - Overview of all stored memories and statistics

#### CLI Usage

```bash
# Run in stdio mode (for Claude Desktop and other MCP clients)
agent-memory-mcp --stdio

# Use different storage backends
agent-memory-mcp --stdio --store-type pinecone --index-name my-memories
agent-memory-mcp --stdio --store-type postgresql --connection-string "postgresql://user:pass@localhost:5432/memories"
```

#### Advanced Configuration

For different storage backends, use the advanced configuration:

```json
{
  "mcpServers": {
    "agent-memory-os-pinecone": {
      "command": "agent-memory-mcp",
      "args": [
        "--stdio",
        "--store-type",
        "pinecone",
        "--index-name",
        "agent-memory-os"
      ],
      "env": {
        "PINECONE_API_KEY": "your-pinecone-api-key-here",
        "PINECONE_ENVIRONMENT": "your-pinecone-environment-here"
      }
    },
    "agent-memory-os-postgresql": {
      "command": "agent-memory-mcp",
      "args": ["--stdio", "--store-type", "postgresql"],
      "env": {
        "POSTGRESQL_CONNECTION_STRING": "postgresql://user:password@localhost:5432/agent_memory"
      }
    }
  }
}
```

#### Try the MCP Demo

```bash
# Run the MCP server demo
python examples/mcp_server_demo.py

# Run in stdio mode for Claude Desktop
python examples/mcp_server_demo.py --stdio
```

**MCP Demo Features:**

- 🧠 **Memory Management**: Create, search, and manage memories through MCP
- 🔗 **Claude Desktop Integration**: Works seamlessly with Claude Desktop
- 📊 **Multiple Storage Backends**: SQLite, Pinecone, and PostgreSQL support
- 🌐 **HTTP and stdio modes**: Flexible deployment options
- 🛠️ **Rich Tool Set**: Complete memory operations through MCP tools

### Run Demos

```bash
# All demos can be run from any directory - they'll automatically find the project root

# Core memory demo (SQLite backend)
python examples/crewai_memory_demo.py

# LangChain integration demo
python examples/langchain_memory_demo.py

# LangGraph integration demo
python examples/langgraph_memory_demo.py

# MCP server demo (for Claude Desktop integration)
python examples/mcp_server_demo.py

# REST API demo (tests HTTP client and async client)
python examples/api_demo.py

# Web UI demo (creates sample data and launches web interface)
python examples/web_ui_demo.py

# Pinecone integration demo (requires Pinecone credentials)
python examples/pinecone_memory_demo.py

# PostgreSQL integration demo (requires PostgreSQL server)
python examples/postgresql_memory_demo.py
```

**Demo Features:**

- 🧠 **Core Demo**: Basic memory operations with SQLite
- 🔗 **LangChain Demo**: Memory-aware chains, tools, and agents
- 📊 **LangGraph Demo**: Memory integration with workflows and state
- 🔌 **MCP Demo**: Claude Desktop integration with memory tools
- 🌐 **API Demo**: HTTP client, async client, and REST endpoints
- 🎨 **Web UI Demo**: Interactive web interface with sample data
- 🌲 **Pinecone Demo**: Vector search with semantic embeddings
- 🐘 **PostgreSQL Demo**: Full-text search with relational database

### Debug and Maintenance Tools

```bash
# Debug Pinecone configuration
python debug_pinecone.py

# Debug Pinecone search functionality
python debug_pinecone_search.py

# Clean up old Pinecone indexes
python cleanup_pinecone.py
```

### Run Tests

```bash
python -m pytest tests/ -v
```

### Troubleshooting

**Common Issues:**

1. **"run_api.py not found" error**

   - ✅ **Fixed**: All demo scripts now automatically find the project root
   - Run demos from any directory: `python examples/web_ui_demo.py`

2. **Pinecone connection errors**

   - Set environment variables: `export PINECONE_API_KEY="your-key"`
   - Use supported environment: `export PINECONE_ENVIRONMENT="gcp-starter"`
   - Run debug script: `python debug_pinecone.py`

3. **PostgreSQL connection errors**

   - Install driver: `pip install psycopg2-binary`
   - Set environment variables: `export POSTGRESQL_HOST="localhost"`
   - Create database: `createdb agent_memory`
   - Check connection: `psql -h localhost -U postgres -d agent_memory`

4. **API server startup errors**

   - ✅ **Fixed**: MemoryManager initialization now properly handles store types
   - Ensure you're in the project root: `python run_api.py --host 127.0.0.1 --port 8000`

5. **Memory search returning 0 results**
   - Check if memories were saved successfully
   - Run debug script: `python debug_pinecone_search.py`
   - Clean up old indexes: `python cleanup_pinecone.py`

---

## 🔗 Status

🚀 **MVP Complete + Full Framework Integration + Vector Search** — Core memory system is working with SQLite and Pinecone storage, comprehensive LangChain, LangGraph, and REST API integrations, persistent memory across sessions, and high-quality semantic search capabilities.

**Current Capabilities:**

- ✅ Persistent memory storage and retrieval
- ✅ LangChain integration with chains, tools, and agents
- ✅ LangGraph integration with workflows, state management, and tools
- ✅ MCP integration for Claude Desktop and other AI assistants
- ✅ REST API for remote memory access (HTTP, Python, async)
- ✅ Web UI for memory visualization and management
- ✅ Memory-aware responses with context
- ✅ Timeline and semantic search
- ✅ Cross-session memory persistence
- ✅ Agent-specific memory isolation
- ✅ **Vector similarity search** with Pinecone integration
- ✅ **Model-based embeddings** for high-quality semantic search
- ✅ **Robust demo scripts** that work from any directory
- ✅ **Comprehensive debugging tools** for troubleshooting

**Next Milestones:**

1. Add CrewAI integration
2. Implement memory compression and summarization
3. Add memory importance scoring

---

## 📬 Stay in the Loop

Want to contribute or test it early? Reach out or open an issue!
